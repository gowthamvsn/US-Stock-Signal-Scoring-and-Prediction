
import joblib

start_time = time.time()

# === CONFIG ===
TODAY = datetime.today()
TODAY_STR = TODAY.strftime("%Y-%m-%d")
now_hour = TODAY.hour
expected_latest = (TODAY - BDay(1)).date() if now_hour < 15 else TODAY.date()
print(" prediction to be date " , expected_latest)
TICKER_FILE = r"D:\\tickproject\\ticker texts\\final_filtered_tickers.txt"
MASTER_FILE = rf"D:\\tickproject\\stock_data\\tickersdetails_US_master.csv"
START_DATE = "2024-10-01"
END_DATE_FULL = (TODAY + timedelta(days=0)).strftime('%Y-%m-%d')
print("this is the date till which im going to download", END_DATE_FULL)
END_DATE_ANALYSIS = (TODAY - BDay(9)).date().strftime('%Y-%m-%d')
print("this is the date till which analysis model is built",END_DATE_ANALYSIS)
FUTURE_END = expected_latest
PREDICT_DATE_START = "2025-04-01"
PREDICT_DATE_END = expected_latest

# === INDICATOR FUNCTION ===
def compute_indicators(df):
    if len(df) > 14:
        try:
            df['DollarVolume'] = df['Close'] * df['Volume']
            df['Log_Close'] = np.log(df['Close'])
            df['Log_BB_Lower'] = volatility.BollingerBands(df['Log_Close']).bollinger_lband()
            df['GK_Volatility'] = 0.5 * (np.log(df['High'] / df['Low'])**2) - (2 * np.log(2) - 1) * (np.log(df['Close'] / df['Open'])**2)
            df['RSI'] = momentum.RSIIndicator(df['Close']).rsi()
            df['MACD_diff'] = trend.MACD(df['Close']).macd_diff()
            df['ATR'] = volatility.AverageTrueRange(df['High'], df['Low'], df['Close']).average_true_range()
            df['EMA50'] = df['Close'].ewm(span=50).mean()
            stoch = momentum.StochasticOscillator(df['High'], df['Low'], df['Close'])
            df['Stoch_K'] = stoch.stoch()
            df['Stoch_D'] = stoch.stoch_signal()
            df['ADX'] = trend.ADXIndicator(df['High'], df['Low'], df['Close']).adx()
            df['ROC'] = momentum.ROCIndicator(df['Close'], window=10).roc()
            df['CCI'] = trend.CCIIndicator(df['High'], df['Low'], df['Close'], window=20).cci()
            df['OBV'] = volume.OnBalanceVolumeIndicator(df['Close'], df['Volume']).on_balance_volume()
            bb = volatility.BollingerBands(df['Close'])
            df['BB_Width'] = bb.bollinger_hband() - bb.bollinger_lband()
        except Exception as e:
            print("âš ï¸ Indicator calculation error:", e)
            return pd.DataFrame()
        return df
    return pd.DataFrame()

# === FETCH TICKERS ===
with open(TICKER_FILE) as f:
    tickers = [line.strip().upper() for line in f if line.strip()]

# === LOAD EXISTING MASTER FILE ===
if os.path.exists(MASTER_FILE):
    df_master = pd.read_csv(MASTER_FILE, parse_dates=["Date"])
    last_date = df_master["Date"].max()
    print(f"ðŸ“‚ Existing data till {last_date.date()}")
    if last_date.date() >= expected_latest:
        print(f"âœ… Already up to date. Data ends at {last_date.date()}, expected {expected_latest}")
    START_DATE = (last_date + timedelta(days=1)).strftime('%Y-%m-%d')
else:
    df_master = pd.DataFrame()
    print("ðŸ†• No master file. Starting fresh.")

# === DOWNLOAD NEW DATA ===
def fetch_and_append_new_data(ticker_list, start_date, end_date):
    all_data = []
    for ticker in ticker_list:
        df = yf.download(ticker, start=start_date, end=end_date)
        #print("starting date is ", start_date)
        #print("last date is ", end_date)
        if df.empty or len(df) < 1:
            continue
        if isinstance(df.columns, pd.MultiIndex):
            df.columns = [col[0] for col in df.columns.values]
        try:
            eps = yf.Ticker(ticker).info.get('epsTrailingTwelveMonths', np.nan)
        except:
            eps = np.nan
        df["Ticker"] = ticker
        df["EPS"] = eps
        df.reset_index(inplace=True)
        all_data.append(df)
    return pd.concat(all_data, ignore_index=True) if all_data else pd.DataFrame()

# === RUN DOWNLOAD IF NEEDED ===
if not df_master.empty and df_master["Date"].max().date() >= expected_latest:
    print("ðŸ“‚ No need to download. Using existing master file.")
    df_all = df_master.copy()
else:
    df_new = fetch_and_append_new_data(tickers, START_DATE, END_DATE_FULL)
    if not df_new.empty:
        df_all = pd.concat([df_master, df_new], ignore_index=True)
        df_all.drop_duplicates(subset=["Ticker", "Date"], inplace=True)
        df_all.sort_values(by=["Ticker", "Date"], inplace=True)
        df_all.to_csv(MASTER_FILE, index=False)
        print(f"ðŸ’¾ Master file updated: {MASTER_FILE}")
    else:
        df_all = df_master.copy()
        print("â›” No new data downloaded")
stage1time = time.time()
print("time taken to complete stage 1 :", stage1time - start_time)

df_analysis = df_all[df_all['Date'] <= END_DATE_ANALYSIS].copy()
grouped = df_analysis.groupby('Ticker')
df_with_indicators = pd.concat([compute_indicators(group) for _, group in grouped])


# Add pass/fail condition flags instead of score
flags = []
for ticker, group in df_with_indicators.groupby('Ticker'):
#   latest = group.dropna().iloc[-1] if not group.dropna().empty else pd.Series()
    required_cols = ['RSI', 'MACD_diff', 'Close', 'EMA50', 'DollarVolume', 'Volume', 'EPS','GK_Volatility', 'ATR', 'Log_Close', 'Log_BB_Lower', 'Stoch_K', 'Stoch_D', 'ADX', 'ROC', 'CCI', 'OBV', 'BB_Width'] #
    # missing_cols = [col for col in required_cols if col not in group.columns]
    # if missing_cols:
    #     print(f"\nâš ï¸ Skipping {ticker}: missing columns {missing_cols}")
    #     print(group[['Date'] + [col for col in required_cols if col in group.columns]].tail(3))
    #     continue

    group_clean = group.dropna(subset=required_cols)
    if group_clean.empty:
        continue
    latest = group_clean.iloc[-1]
    if not latest.empty:
    # latest = group.iloc[-1]
        result = {
        'Ticker': ticker,
        'RSI_under_30': int(latest['RSI'] < 30),
        'MACD_positive': int(latest['MACD_diff'] > 0),
        'Close_above_EMA50': int(latest['Close'] > latest['EMA50']),
        'High_DollarVolume': int(latest['DollarVolume'] > group['DollarVolume'].rolling(20).mean().iloc[-1]),
        'EPS_positive': int(pd.notna(latest['EPS']) and latest['EPS'] > 0),
        'High_GK_Volatility': int(latest['GK_Volatility'] > group['GK_Volatility'].median()),
        'ATR_above_avg': int(latest['ATR'] > group['ATR'].rolling(20).mean().iloc[-1]),
        'Log_Close_above_BB_Lower': int(latest['Log_Close'] > latest['Log_BB_Lower']),
        'Stoch_K_crossover': int(group['Stoch_K'].iloc[-2] < group['Stoch_D'].iloc[-2] and latest['Stoch_K'] > latest['Stoch_D'] and latest['Stoch_K'] < 20),
        'ADX_above_20': int(latest['ADX'] > 20),  # Strong trend
        'ROC_above_2': int(latest['ROC'] > 2),    # Price gain acceleration
        'CCI_extreme': int(abs(latest['CCI']) > 100),  # Overbought or oversold
        'OBV_rising': int(latest['OBV'] > group['OBV'].rolling(5).mean().iloc[-1]),
        'BB_width_expansion': int(latest['BB_Width'] > group['BB_Width'].rolling(20).mean().iloc[-1])
    
    }
    flags.append(result)
stage2time = time.time()
print("time taken to complete stage 2 ", stage2time - stage1time)

# === STAGE 3: Add Target Variable from Future Data ===
df_future = df_all[(df_all['Date'] >= pd.to_datetime(END_DATE_ANALYSIS)) &(df_all['Date'] <= pd.to_datetime(FUTURE_END))].copy()
future_gains = []

for ticker, group in df_analysis.groupby('Ticker'):
    # print(f"\nðŸ” Processing {ticker}")
    future_group = df_future[df_future['Ticker'] == ticker]
    # print(f"â†’ Future group rows: {len(future_group)}")
    if future_group.empty:
        # print("âš ï¸ Future group is empty")
        gain = np.nan
    else:
        current_close = group[group['Date'] == pd.to_datetime(END_DATE_ANALYSIS)]['Close']
        # print(f"â†’ Current close on {END_DATE_ANALYSIS}: {current_close.values}")
        if current_close.empty:
            print("âš ï¸ No current close price found for END_DATE_ANALYSIS")
            gain = np.nan
        else:
    #         future_max = future_group['Close'].max()
    #         gain = (future_max - current_close.values[0]) / current_close.values[0]
    # future_gains.append({'Ticker': ticker, 'FutureGain_5d': gain})
            future_max_row = future_group.loc[future_group['Close'].idxmax()]
            future_max = future_max_row['Close']
            future_max_date = future_max_row['Date']
            gain = (future_max - current_close.values[0]) / current_close.values[0]
            # print(f"âœ… Max future close: {future_max} on {future_max_date}, Gain: {gain:.2%}")
    future_gains.append({'Ticker': ticker, 'FutureGain_5d': gain, 'PeakDate': future_max_date})


flags_df = pd.DataFrame(flags)
#flags_df.to_csv(rf"D:\tickproject\stock_data\condition_flags_{TODAY}.csv", index=False)
#print("âœ… Saved indicator condition flags")
#df_with_indicators.to_csv(rf"D:\tickproject\stock_data\technical_indicators_{TODAY}.csv", index=False)
#print("âœ… Saved technical indicators")

future_df = pd.DataFrame(future_gains)
flags_df = flags_df.merge(future_df, on='Ticker', how='left')
flags_df['Target'] = (flags_df['FutureGain_5d'] >= 0.06).astype(int)

print(" lets see how target is ")
print(flags_df['FutureGain_5d'].describe())

#flags_df.to_csv(rf"D:\tickproject\stock_data\condition_flags_with_target_{TODAY}.csv", index=False)
stage3time = time.time()
print(" time taken to complete stage 3 is ", stage3time  - stage2time)
# # === STAGE 4: Correlate Indicator Flags with Target ===
# correlation_output_path = rf"D:\tickproject\stock_data\correlation_matrix_{TODAY}.csv"
correlation = flags_df.drop(columns=['Ticker', 'PeakDate', 'FutureGain_5d']).corr()
#correlation['Target'].sort_values(ascending=False).to_csv(correlation_output_path)
# print("âœ… Saved correlation results")

stage4time = time.time()
print(" time taken to complete stage 4 ", stage4time - stage3time)
# === STAGE 5: Generate Weights Based on Correlation ===
weights_path = rf"D:\tickproject\stock_data\indicator_weights_{TODAY_STR}.csv"
target_corr = correlation['Target'].drop('Target')

def assign_weight(c):
    if c >= 0.20:
        return 3
    elif c >= 0.10:
        return 2
    elif c >= 0.05:
        return 1
    elif c <= -0.21:
        return -3
    elif c <= -0.11:
        return -2
    elif c <= -0.06:
        return -1
    else:
        return 0

weight_df = target_corr.apply(assign_weight).reset_index()
weight_df.columns = ['Feature', 'Weight']
weight_df.to_csv(weights_path, index=False)
print("âœ… Saved feature weights based on correlation")

stage5time = time.time()
print(" time taken to complete stage 5 ", stage5time - stage4time)

# === STAGE 6: Weighted Scoring ===
weights_df = pd.read_csv(weights_path)
weights_dict = dict(zip(weights_df['Feature'], weights_df['Weight']))

# Apply weighted score
def compute_weighted_score(row):
    return sum(row[feat] * weights_dict.get(feat, 0) for feat in weights_dict)

flags_df['Score'] = flags_df.apply(compute_weighted_score, axis=1)
flags_df.sort_values('Score', ascending=False).to_csv(rf"D:\tickproject\stock_data\scored_tickers_{TODAY_STR}.csv", index=False)
print("âœ… Saved final weighted scores")

stage6time = time.time()
print(" time taken to complete stage 6 ", stage6time - stage5time)

# === STAGE 7: Train and Evaluate ML Model ===
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
import seaborn as sns

ml_df = flags_df.dropna(subset=['Target','FutureGain_5d','PeakDate']).copy()
#print("columns of ml df ", ml_df.columns)
feature_cols = [col for col in weights_dict if col in ml_df.columns]
#print(" feature cols before training with ML", feature_cols)
X = ml_df[feature_cols]
y = ml_df['Target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)
#print("y train values ", y_train.value_counts())

model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("ðŸ“Š Classification Report:")
print(classification_report(y_test, y_pred))

stage7time = time.time()
print(" time taken to complete stage 7 ", stage7time - stage6time)


# # === STAGE 8: Visualize Score Distribution vs. Target ===
# sns.boxplot(data=ml_df, x='Target', y='Score')
# plt.title("Score Distribution by Target")
# plt.savefig(rf"D:\tickproject\stock_data\score_vs_target_{TODAY}.png")
# print("âœ… Saved boxplot: score vs. target")


stage8time = time.time()
print(" time taken to complete stage 8 ", stage8time - stage7time)


# === STAGE 9: Save model for future predictions ===
import joblib
model_path = rf"D:\tickproject\stock_data\rf_model_{TODAY_STR}.pkl"
joblib.dump(model, model_path)
print(f"âœ… Saved trained model to {model_path}")


# === STAGE 10: Predict on New Tickers Using Saved Model ===
from joblib import load

MODEL_FILE = model_path  # already saved as rf_model_<TODAY>.pkl
PREDICT_OUTPUT = rf"D:\tickproject\stock_data\predictions_US_{TODAY_STR}.csv"

print("ðŸ”® Predicting on new data...")

# Load model
predict_model = load(MODEL_FILE)

# Filter for post-prediction data
df_future_predict = df_all[(df_all['Date'] >= pd.to_datetime(PREDICT_DATE_START)) & (df_all['Date'] <= pd.to_datetime(PREDICT_DATE_END))].copy()
#print("future dataframe is ", df_future_predict)
# Compute indicators for prediction
# for ticker, group in df_future_predict.groupby('Ticker'):
#     print(f"{ticker}: {len(group)} rows")

df_future_indicators = pd.concat([compute_indicators(group) for _, group in df_future_predict.groupby('Ticker')])
#print(" future indicators columns are ",df_future_indicators.columns )
# Generate flags as usual
prediction_flags = []
for ticker, group in df_future_indicators.groupby('Ticker'):
    group = group.copy()
    # if group.shape[0] < 5:
    #     continue
    group = compute_indicators(group)
#    print(f"{ticker} â†’ computed {group.shape[0]} rows with indicators")

    latest = group.iloc[-1]
    # if any(pd.isna(latest[col]) for col in required_cols if col in group.columns):
    #     print(f"âš ï¸ Skipping {ticker}: latest row has NaNs in indicators")
    #     continue

    # if not all(col in group.columns for col in required_cols):
    #     print(f"â›” {ticker} missing some required columns.")
    #     continue

    # group = group.tail(20).dropna()
    # if group.empty:
    #     continue
    # latest = group.iloc[-1]

    #required_cols = ['RSI', 'MACD_diff', 'Close', 'EMA50', 'DollarVolume', 'Volume', 'EPS', 'GK_Volatility', 'ATR', 'Log_Close', 'Log_BB_Lower', 'Stoch_K', 'Stoch_D']
   # if not all(col in group.columns for col in required_cols):
    #    continue
    #print("group is ", group)
    #group_clean = group.dropna(subset=required_cols)
    #print(" group clean is ", group_clean)
    #if group_clean.empty:
     #   continue
    #latest = group_clean.iloc[-1]
#    print("latest is ", latest)
    if not latest.empty:
        result = {
            'Ticker': ticker,
            'RSI_under_30': int(latest['RSI'] < 30),
            'MACD_positive': int(latest['MACD_diff'] > 0),
            'Close_above_EMA50': int(latest['Close'] > latest['EMA50']),
            'High_DollarVolume': int(latest['DollarVolume'] > group['DollarVolume'].rolling(20).mean().iloc[-1]),
            'EPS_positive': int(pd.notna(latest['EPS']) and latest['EPS'] > 0),
            'High_GK_Volatility': int(latest['GK_Volatility'] > group['GK_Volatility'].median()),
            'ATR_above_avg': int(latest['ATR'] > group['ATR'].rolling(14).mean().iloc[-1]),
            'Log_Close_above_BB_Lower': int(latest['Log_Close'] > latest['Log_BB_Lower']),
            'Stoch_K_crossover': int(group['Stoch_K'].iloc[-2] < group['Stoch_D'].iloc[-2] and latest['Stoch_K'] > latest['Stoch_D'] and latest['Stoch_K'] < 20),
            'ADX_above_20': int(latest['ADX'] > 20),  # Strong trend
            'ROC_above_2': int(latest['ROC'] > 2),    # Price gain acceleration
            'CCI_extreme': int(abs(latest['CCI']) > 100),  # Overbought or oversold
            'OBV_rising': int(latest['OBV'] > group['OBV'].rolling(5).mean().iloc[-1]),
            'BB_width_expansion': int(latest['BB_Width'] > group['BB_Width'].rolling(20).mean().iloc[-1])
        
        }
        #future_flags.append(result)
        prediction_flags.append(result)

future_flags_df  = pd.DataFrame(prediction_flags)
#print("future df is ",future_flags_df)
model = load(model_path)
predict_feature_cols = [
'RSI_under_30', 'MACD_positive', 'Close_above_EMA50',
'High_DollarVolume',  'EPS_positive','High_GK_Volatility',
'ATR_above_avg', 'Log_Close_above_BB_Lower', 'Stoch_K_crossover', 'ADX_above_20',
'ROC_above_2',
'CCI_extreme',
'OBV_rising',
'BB_width_expansion'
]# 

future_flags_df ['Predicted_Prob'] = model.predict_proba(future_flags_df [predict_feature_cols])[:, 1]
#future_flags_df ['Predicted_Prob'] = model.predict_proba(future_flags_df [predict_feature_cols])

print("Proba columns:", future_flags_df.columns)
print("Proba shape:", future_flags_df.shape)
#print("Proba values sample:\n", future_flags_df[:50])


future_flags_df.sort_values('Predicted_Prob', ascending=False).to_csv(rf"D:\tickproject\stock_data\predictions_US_{TODAY_STR}.csv", index=False)
print("âœ… Saved predictions on new data")

#future_flags_df = pd.DataFrame(future_flags)
#print("future flag df is ", future_flags_df.head(5))
# Predict using the model
# X_new = future_flags_df[feature_cols]
# future_flags_df['Predicted_Prob'] = predict_model.predict_proba(X_new)[:, 1]

# predict_feature_cols = [
#     'RSI_under_30', 'MACD_positive', 'Close_above_EMA50',
#     'High_DollarVolume', 'EPS_positive', 'High_GK_Volatility',
#     'ATR_above_avg', 'Log_Close_above_BB_Lower', 'Stoch_K_crossover'
# ]

# future_flags_df['Predicted_Prob'] = model.predict_proba(future_flags_df[predict_feature_cols])[:, 1]



# # Save predictions
# future_flags_df.sort_values('Predicted_Prob', ascending=False).to_csv(PREDICT_OUTPUT, index=False)
# print(f"âœ… Saved predictions to {PREDICT_OUTPUT}")



end_time = time.time()
print("overall time taken is : ", end_time - start_time )
